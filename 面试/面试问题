


# Agent & RAG & prompt




## 什么是agent 


- [能大致讲一下agent的原理吗？](https://www.zhihu.com/question/4838236459/answer/1951966477113602156)


**智能体 就是围绕大模型构建的一套“目标导向、具备记忆、能自主调用工具”的系统** , 构建智能体的目标是希望其可以实现多步的复杂任务，自主决策，

但目前agent ， 既缺乏复杂任务的多步决策，也缺乏记忆能力， 目前只能实现调用工具，但在怎么调用工具上，缺少智能， 即绝大部分agent应用最后都变成了规则驱动的
## 你有没有写prompt 的自我心得 和 经验

1. 提炼你的描述，有一个关键简短的话，再加上详细的描述，而不是直接塞入文本描述，尤其是一些任务步骤描述，任务背景下


1. 从模型的输出中反馈，当模型的输出没有按照你预想的结果的化，站在 模型的角度取思考为什么是这个结果

1. 结构化的prompt, 使用markdown格式，或xml格式，对于关键概念加粗，对于大段文本使用xml的尖括号包裹 


2 输出也得是结构化的，通过结构化的输出直接约束模型的输出空间，提升任务的可靠性与稳定性，结构化格式确实能在更少token里塞进更多信息：



## 在一个ai驱动的系统，你有哪些经验


对于这个问题可以看claude 和 devin公司的博客，以及一些构建智能体的博客

此外还有些的别的回答


```bash


如果你想开发生产级的Agent，就要切记： 
- 微智能体优于巨型循环：保持任务聚焦（5-10步）以避免上下文漂移 
- 结构优于自由形式：使用JSON Schema和类型化输出而非自由文本 
- 确定性编排：让代码控制流程，LLM处理决策 统一状态管理：避免复杂的状态分离抽象 
- 可中断执行：在工具选择和执行之间提供审批点 
- 智能错误处理：利用上下文进行自愈，但设置合理限制

虽然模型会越来越强，成本会越来越低，但我相信核心原则不会变：控制、可观测性和可靠性
控制、可观测性和可靠性。
```

以上内容来自[link](https://www.zhihu.com/question/1891113677635162799/answer/1955525874859372835)







## agent 出现循环调用工具，卡死， 认为原因， 解决办法


原因

1. 任务过于复杂，需多次调用工具，llm调用工具后，迷失任务目标

1. 工具输出内容过多
2. 现有的模型能力不足，并不具备超长上下文和 多步推理能力

1. llm反复调用一个工具，即使工具返回的结果不对，

## agent 和 workflow 的区别是什么呢


- [answer](https://www.zhihu.com/question/1896707093580448857/answer/1954690368198117063)


## 能说一下react是指什么呢，

- [Agent开发(一) - ReACT框架详解](https://zhuanlan.zhihu.com/p/1954839068434208201)


“Thought → Action → Observation” 的循环。这个格式强制或鼓励模型在生成动作之前或之后插入思考步骤。




## agent 常见的几种设计模式


- [智能体工作流的六种设计模式](https://zhuanlan.zhihu.com/p/1941170327955671029)

- [Agent的九种设计模式(图解+代码)](https://zhuanlan.zhihu.com/p/692971105)


## 多Agent 为什么效果不好





## Agent 落地问题

- agent的能力（基于llm更多选择权和能力 ) 与 可控性不足的问题
- 复杂问题agent 无法胜任  退化为了 workflow 
- 单个agent一次只做一件事，多个步骤任务拆成多步，采用链式路径
- 



## 多agent 如何设计

- [关于 multi-agent 的一些记录](https://zhuanlan.zhihu.com/p/1954144220060313488)

- [Claude Code Sub-agent 模式的详解和实践](https://zhuanlan.zhihu.com/p/1940513054916875486)


## n


## 阿里的deepresearch 的全流程

- [DeepResearch调研](https://zhuanlan.zhihu.com/p/1953917263267758677)


## 你对代码仓下agent 或者 swe agent 的看法

1. 不要和模型能力发展趋势做对： 例如构建复杂的符号知识图谱，使用大量的向量检索 检索很难匹配的上，长上下文，模型代码的理解能力都在提高， 另一方面 向量检索 不可能精准（在大规模代码仓，多个代码仓，等等，其最终退化为半关键字检索） ， 符号调用知识图谱不可靠 

1. 代码问题因为有可验证的解， 因此在agent应用上也将很可能成为快速被攻破的地方，目前的AI的算法竞赛已经快要超越人类选手，下一个阶段将会是在可验证的 代码仓 编程上, 在一个提供基础的开发环境上（ 环境的配置可以通过docker ， 加上一部分联网搜索等）** 借助强化学习**
实现swe ，实现复杂的 代码库重构，issue解决，需求开发任务


包括
    - 1. 自动记录repo内容为自己的检索来源 增量式的加深自己对repo的理解

    - 2. 自己维护一个自己的上下文窗口

    - 3. 自己维护一个笔记记录，不断的更新， 打标签，将标签常驻内存，


当然对于repo还有
    - check 文件
    - 通过搜索或ctag实现人类类似跳转的能力
    

reward：
    1. 记录内容，且 记录精准
    2. 主动更新内容
    3. 写出通过编译的代码
    4. 

# 机器学习 or 深度学习


## 常用的优化器有哪些 ，原理or各自的优劣性


## 一个机器学习or深度学习网络的实现步骤包括哪些





# 模型架构

## 位置编码 rope



- [浅谈位置编码（Positional Encoding, PE）：Absolute PE +RoPE+ALiBi](https://zhuanlan.zhihu.com/p/1955755129568469592)

## encoder 和 decoder都是可以并行的么



## 模型的外推性能长度是由哪几个方面影响的

## 为什么现在的LLM都是Decoder-only的架构？ 

参考苏剑林博客


## 大模型的 Embedding 层和独立的 Embedding 模型有什么区别？

- [answer](https://www.zhihu.com/question/663835334/answer/1955795278595613256)

# 为什么transformer的FFN需要先升维再降维？

- [answer](https://www.zhihu.com/question/665731716/answer/3611802917)

## llm训练和推理阶段的 输入到模型输出的全过程 包括最终loss计算






## Query 和 Key 在注意力机制中长得几乎一模一样，为什么还要分开? 或者 说 为什么要设置QKV三个矩阵来进行注意力的计算，退化成Q 和 V 或者只有Q和K呢


- [知乎答案](https://www.zhihu.com/question/1934742507746464833/answer/1956361740917912910)



## deepseek r1 模型架构有哪些改动




## RMS norm , layer norm 等 llm中采用的正则化方法


## 分词计算 ，BPE算法

## LLM的激活函数的变化


## 从MHA、MQA、GQA到MLA

[苏剑林博博客](https://zhuanlan.zhihu.com/p/700588653)

## transformer中使用的position embedding为什么是加法？

- [参考苏剑林的博客]


## LLM的padding 

左填充还是右填充

- [LLM padding 细节](https://zhuanlan.zhihu.com/p/675273498)


## 从Token到Logits ： LLM中的计算



- [LLM计算得到的logits或者token在计算CELoss的时候是怎么计算的。](https://zhuanlan.zhihu.com/p/1953853447691547952)


- [LLM(decoder-only)在inference的时候可微吗？ 答案 不可以，因为argmax会断开梯度(https://zhuanlan.zhihu.com/p/1953853447691547952)

## 训练参数

- [梯度检查/梯度累积 : LLM 梯度检查点（Gradient Checkpointing）：一文掌握](https://zhuanlan.zhihu.com/p/1954366455685583235)




# 训练和推理


## 训练时候的流水并行， 

- [模型并行训练策略：张量并行、流水线并行与混合并行](https://zhuanlan.zhihu.com/p/1955934185551291190)

## sft的计算公式什么
```bash

loss = loss * torch.softmax(shift_logits, dim=-1).gather(1, shift_labels.unsqueeze(-1)).squeeze(-1).detach()





```


# RL 强化学习


## 教程入门
- [**图解大模型RLHF系列之：人人都能看懂的PPO原理与源码解读**](https://zhuanlan.zhihu.com/p/677607581)
- [人人都能看懂的RL-PPO理论知识](https://zhuanlan.zhihu.com/p/7461863937)

- 强化学习的基础知识
	- [通俗解读强化学习中常用的三大采样方法](https://zhuanlan.zhihu.com/p/26576501447)


##  什么是奖励劫持 
```bash

。理论上，RL 可以应用到任何场景，但如果你的判断能力不强，智能体学到的可能只是噪声。

RL 的一个直接限制就是它可能学到你不希望出现的行为：RL 会尽一切手段去最大化你给予的奖励，所以模型会做你要求它做的事情，而不是你真正想要的。我们称这种现象为奖励劫持（reward hacking），虽然模型本身无辜，但设计奖励机制的人需要承担责任。

一个典型例子：RL 发现《超级马里奥 1》存在一个 30 多年的漏洞：跳跃后，如果你转身，你会在一帧内无敌。由于 RL 的奖励是最大化得分，而更快通关可以获得更高分数，它就利用这个漏洞疯狂刷高分，从而获取更多奖励。

开发者的期望：最大化得分；保持像人类一样玩；不利用漏洞
开发者实际给的指令：最大化得分
顺便说一句，这种情况在人类身上也会发生！我们称之为反向激励（Perverse Incentives），本质上和 RL 的奖励劫持是同一个道理。一个著名案例是：英国政府为控制德里毒蛇数量，对每条死蛇提供奖金。起初效果很好，大量毒蛇被杀。但后来人们开始专门养蛇以赚取奖金。
```


# RLHF 



## PPO详解

## ppo算法 手推 和 代码

- [PPO算法逐行代码详解](http://zhuanlan.zhihu.com/p/660971357)

参考[从0开始手写PPO算法（二）](https://zhuanlan.zhihu.com/p/577482337)




## 有做过dpo么



## 什么是 ：On-policy 与 Off-policy

## DPO PPO 概念差异



## grpo 和 ppo 差异


## 其他 的rl算法的变种


- [PPO、DPO、GRPO及其变体（Dr. GRPO、DAPO、GSPO、GMPO、GFPO、LitePPO）策略优化算法综述](https://zhuanlan.zhihu.com/p/1946231150822552323)

## 提问：PPO中为什么要做clip ratio，主要考虑了哪些因素？

- [answer](https://zhuanlan.zhihu.com/p/25751312482)



## RL为什么有效
- [RL为什么有效?
 个人认为, RL 与 SFT 的区别在于, SFT 是 token level 的 0/1 奖励, RL 是句子 level 的离散奖励.](https://zhuanlan.zhihu.com/p/26370587517)







## RL中的KL散度


- [KL项起什么作用?](https://zhuanlan.zhihu.com/p/26370587517)

- [在TRPO等类似策略优化方法中，为什么使用「新策略相对于旧策略的KL散度」，而不是反过来？](https://www.zhihu.com/question/10462758287/answer/1956099807778145503)

## GRPO算法是怎样的， grpo和ppo有什么区别和改进
- [GRPO算法与 PPO 算法的本质区别是什么？该如何选择？](https://www.zhihu.com/question/12933942086/answer/1919851560382464325)


## 为什么GRPO很容易训飞，训到一半reward就很容易突然掉下来？





# 推理

## KV cache & 算子  && llm推理全过程


- [图解LLM推理流程---Attention算子]


## 为什么可以进行k v cache




## flash attention





# RAG & 问答

## Elsatci search 用过么，说一下


## 构建问答系统包括哪些呢

- [我为什么还在用“过时”的BERT + Faiss + ES做问答系统（并且它跑得比GPT还稳）](https://zhuanlan.zhihu.com/p/1911485849956225878)



## 你觉得rag问答应用的主要问题 和 瓶颈 是什么，或者说你自己做项目中遇到的显著问题是什么



- 问题路由--意图识别
    - 多轮对话的意图转变
    
- 用户问题的表达简略性 让LLM难以一次性直接定位用户意图

## 如何做好问答系统的路由


- [AI 智能体实战：100+次迭代后的意图识别提升之道](https://zhuanlan.zhihu.com/p/1940769657406624577)


## 如何在不微调的情况下提高 RAG 的准确性？ 


1. 用llm改进query
2. 用llm改进索引
3. llm主动发起query
4. bm25 辅助正则查询 
5.

## RAG未来会消亡么，


- [RAG未来的出路](https://zhuanlan.zhihu.com/p/703882032)


